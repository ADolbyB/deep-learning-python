{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks Part 2:\n",
    "\n",
    " - Methods To Handle Overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "\n",
    " - https://keras.io/applications/\n",
    " - https://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d\n",
    " - https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/\n",
    " - https://neurohive.io/en/popular-networks/vgg16/\n",
    " - https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/\n",
    " - https://github.com/Xinyi6/CIFAR10-CNN-by-Keras/blob/master/doc/CIFAR10%20CNN%20use%20Keras.pdf\n",
    " - https://github.com/moritzhambach/Image-Augmentation-in-Keras-CIFAR-10-/blob/master/CNN%20with%20Image%20Augmentation%20(CIFAR10).ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn Off Messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Fix: UnknownError: Failed to get convolution algorithm. \n",
    "# This is probably because cuDNN failed to initialize, \n",
    "# so try looking to see if a warning log message was printed above\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check GPU Availibility & Set Memory Limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1792)]) # Alt: 1792 MB, 2048 MB\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, Activation, MaxPooling2D, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions For Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training accuracy and loss\n",
    "def plotCurve(trainingAccuracy, trainingLoss):\n",
    "    epochs = np.arange(trainingLoss.shape[0])\n",
    "    #print(epochs)\n",
    "    plt.figure(figsize = [12, 6])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, trainingAccuracy)\n",
    "    #plt.axis([-1, 2, -1, 2])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, trainingLoss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary CrossEntropy Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "def accuracyFunc(labelsActual, labelsPred):\n",
    "    accuracy = (np.sum(labelsActual == labelsPred) / len(labelsActual)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Plot Features of CIFAR-10 data\n",
    "def feat_plot(features, labels, classes, title):\n",
    "    for class_i in classes:\n",
    "        plt.plot(features[labels[:] == classes[class_i], 0],\n",
    "                 features[labels[:] == classes[class_i], 1], 'o', markersize = 5)\n",
    "        # plt.axis([-2, 2, -2, 2])\n",
    "        plt.xlabel('X: Feature 1')\n",
    "        plt.ylabel('Y: Feature 2')\n",
    "        plt.title(title)\n",
    "        plt.legend(['Class ' + str(classes[class_i]) for class_i in classes])\n",
    "    plt.show()\n",
    "\n",
    "def plotBothCurves(trainingAccuracy, trainingLoss, validAccuracy, validLoss):\n",
    "    epochs = np.arange(trainingLoss.shape[0])\n",
    "    epochsSaved = np.arange(validLoss.shape[0])\n",
    "    #print(epochs)\n",
    "    # 1st Subplot: Accuracy\n",
    "    plt.figure(figsize = [12, 6])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, trainingAccuracy)\n",
    "    plt.plot(epochsSaved, validAccuracy) # Saved Best Value\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    # 2nd Subplot: Training Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, trainingLoss)\n",
    "    plt.plot(epochsSaved, validLoss)\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Binary CrossEntropy Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "# Plot CIFAR-10 Data In Subplots\n",
    "def img_plt(img, label):\n",
    "    plt.figure(figsize=(12, 5.5))\n",
    "    for i in range (1, 11):\n",
    "        plt.subplot(2, 5, i)\n",
    "        plt.imshow(img[i - 1, :, :], cmap = 'gray')\n",
    "        plt.title('Label: ' + str(label[i - 1]))\n",
    "    plt.show()\n",
    "\n",
    "def plotEverything(model_hist):\n",
    "    trainAcc = np.array(model_hist.history['accuracy'])\n",
    "    trainLoss = np.array(model_hist.history['loss'])\n",
    "    validAcc = np.array(model_hist.history['val_accuracy'])\n",
    "    validLoss = np.array(model_hist.history['val_loss'])\n",
    "\n",
    "    plotBothCurves(trainAcc, trainLoss, validAcc, validLoss)\n",
    "\n",
    "    trainScore = [trainAcc[-1], trainLoss[-1]]\n",
    "    print(f'\\nTraining Accuracy: {round(trainScore[0], 5) * 100} %')\n",
    "    print(f'Training Loss: {round(trainScore[1], 4)}\\n')\n",
    "\n",
    "    valScore = [validAcc[-1], validLoss[-1]]\n",
    "    print(f'\\nValidation Accuracy: {round(valScore[0], 5) * 100} %')\n",
    "    print(f'Validation Loss: {round(valScore[1], 4)}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data:\n",
    "\n",
    " - Each of the 50,000 Images is 32 x 32 pixels x 3 RGB values\n",
    "     - Each Image is 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "classes = np.arange(10)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomize & Select 20% of Images For Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_img = x_train.shape[0]\n",
    "train_ind = np.arange(0, num_train_img)\n",
    "train_ind_s = np.random.permutation(train_ind)\n",
    "print(train_ind)\n",
    "print(train_ind_s, '\\n')\n",
    "\n",
    "x_train = x_train[train_ind_s,:,:,:]\n",
    "y_train = y_train[train_ind_s]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape, '\\n')\n",
    "\n",
    "x_val = x_train[0:int(0.2 * num_train_img),:,:,:]\n",
    "y_val = y_train[0:int(0.2 * num_train_img)]\n",
    "print(x_val.shape)\n",
    "print(y_val.shape, '\\n')\n",
    "\n",
    "x_train = x_train[int(0.2 * num_train_img):,:,:]\n",
    "y_train = y_train[int(0.2 * num_train_img):]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape, '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 1st 10 Images From Randomized Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_plt(x_train[0:10,:,:,:], y_train[0:10]) # plot the images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling The Images to $ [0, \\ 1] $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_val /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Class Vectors to Binary Class Matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_c = to_categorical(y_train, len(classes))\n",
    "y_val_c = to_categorical(y_val, len(classes))\n",
    "y_test_c = to_categorical(y_test, len(classes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Method: Image-Data Augmentation Only:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2))) # MaxPooling2D Layer or AveragePooling2D Layer\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model4.add(Flatten()) # Optional:Flattens 2D images to 1D Array\n",
    "model4.add(Dense(units=512, activation='relu'))\n",
    "model4.add(Dropout(0.5)) # Helps to reduce overfitting\n",
    "model4.add(Dense(units=len(classes), activation='softmax'))\n",
    "model4.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile The Model:\n",
    "\n",
    " - Create A Data Generator For Real-Time Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt4 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model4.compile(loss='categorical_crossentropy', optimizer=opt4, metrics=['accuracy'])\n",
    "\n",
    "datagen4 = ImageDataGenerator(\n",
    "    #shear_range = -0.1,\n",
    "    #zoom_range = -0.1,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Quantities Required For Featurewise Normalization:\n",
    "\n",
    " - std deviation, mean & principal components if ZCA Whitening is Applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen4.fit(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create A Checkpoint To Save Best Model:\n",
    "\n",
    " - Based On Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path4 = '../assets/Lecture7/model4_ckpt.h5'\n",
    "callbacks_save4 = ModelCheckpoint(save_path4, monitor='val_loss', verbose=0, save_best_only=True, save_freq='epoch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit The Model Using The Data Generator & Plot Statistics:\n",
    "\n",
    "Two Methods To Flow The Images:\n",
    "\n",
    "\n",
    "1. `.flow_from_directory()` Data Generator For Large Datasets:\n",
    "\n",
    "`image_generator = image.datagen.flow_from_directory('data/images', class_mode=None, seed=seed)`\n",
    "\n",
    "\n",
    "2. `.flow()` for small datasets loaded into memory:\n",
    "\n",
    "`model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), steps_per_epoch=len(train) / 32, epochs=epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model4.fit(datagen4.flow(x_train, y_train_c, batch_size=16), \n",
    "                      steps_per_epoch=len(x_train) / 16, epochs=50, \n",
    "                      verbose=1, validation_data=(x_val, y_val_c), \n",
    "                      callbacks=[callbacks_save4])\n",
    "\n",
    "plotEverything(history4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Best Model & Evaluate On Held-Out Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4a = load_model(save_path4)\n",
    "\n",
    "score4a = model4a.evaluate(x_train, y_train_c)\n",
    "print(f'Total Loss From Training Set: {score4a[0]}')\n",
    "print(f'Accuracy From Training Set: {score4a[1]}\\n')\n",
    "\n",
    "score4b = model4a.evaluate(x_val, y_val_c)\n",
    "print(f'Total Loss From Validation Set: {score4b[0]}')\n",
    "print(f'Accuracy From Validation Set: {score4b[1]}\\n')\n",
    "\n",
    "score4c = model4a.evaluate(x_test, y_test_c)\n",
    "print(f'Total Loss From Testing Set: {score4c[0]}')\n",
    "print(f'Accuracy From Testing Set: {score4c[1]}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Class Of Held Out Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class1_prob = model4.predict(x_test)\n",
    "test_labels = np.argmax(test_class1_prob, axis=1)\n",
    "print(f'The Accuracy Predicting The Test Set is: {accuracy_score(test_labels, y_test) * 100} %\\n')\n",
    "conf_matrix = confusion_matrix(test_labels, y_test)\n",
    "\n",
    "colLabels = ['Pred Plane', 'Pred Car', 'Pred Bird', 'Pred Cat', 'Pred Deer',\n",
    "             'Pred Dog', 'Pred Frog', 'Pred Horse', 'Pred Ship', 'Pred Truck']\n",
    "rowLabels = ['Actual Plane', 'Actual Car', 'Actual Bird', 'Actual Cat', 'Actual Deer',\n",
    "             'Actual Dog', 'Actual Frog', 'Actual Horse', 'Actual Ship', 'Actual Truck']\n",
    "\n",
    "fancyMatrix = pd.DataFrame(conf_matrix, columns=colLabels, index=rowLabels,)\n",
    "display(fancyMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Method: Batch Normalization Without Data Augmentation:\n",
    "\n",
    " - Adds a 'normalization layer' after each convolutional layer.\n",
    "     - Between Convolutional Layer and Activation Layer.\n",
    "     - No need for bias in the Convolutional Layer.\n",
    " - Model Converges Much Faster In Training\n",
    "     - Can Use higher Learning Rates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=x_train.shape[1:], use_bias=False))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Conv2D(32, (3, 3), padding='same', use_bias=False))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2))) # MaxPooling2D Layer or AveragePooling2D Layer\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model5.add(Flatten()) # Optional:Flattens 2D images to 1D Array\n",
    "model5.add(Dense(units=512))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.5)) # Helps to reduce overfitting\n",
    "model5.add(Dense(units=len(classes)))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('softmax'))\n",
    "model5.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile The Model:\n",
    "\n",
    " - Increase Learning Rate 10x: 0.001 to 0.01\n",
    " - Increase Batch Size 4x: 16 to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path5 = '../assets/Lecture7/model5_ckpt.h5'\n",
    "callbacks_save5 = ModelCheckpoint(save_path5, monitor='val_loss', verbose=0, save_best_only=True, save_freq='epoch')\n",
    "\n",
    "opt5 = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model5.compile(loss='categorical_crossentropy', optimizer=opt5, metrics=['accuracy'])\n",
    "\n",
    "history5 = model5.fit(x_train, y_train_c, batch_size=64, \n",
    "                      epochs=50, verbose=1, validation_data=(x_val, y_val_c), \n",
    "                      callbacks=[callbacks_save5])\n",
    "\n",
    "plotEverything(history5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Best Model & Evaluate On Held-Out Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5a = load_model(save_path5)\n",
    "\n",
    "score5a = model5a.evaluate(x_train, y_train_c)\n",
    "print(f'Total Loss From Training Set: {score5a[0]}')\n",
    "print(f'Accuracy From Training Set: {score5a[1]}\\n')\n",
    "\n",
    "score5b = model5a.evaluate(x_val, y_val_c)\n",
    "print(f'Total Loss From Validation Set: {score5b[0]}')\n",
    "print(f'Accuracy From Validation Set: {score5b[1]}\\n')\n",
    "\n",
    "score5c = model5a.evaluate(x_test, y_test_c)\n",
    "print(f'Total Loss From Testing Set: {score5c[0]}')\n",
    "print(f'Accuracy From Testing Set: {score5c[1]}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Class Of Held Out Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class2_prob = model5.predict(x_test)\n",
    "test_labels2 = np.argmax(test_class2_prob, axis=1)\n",
    "print(f'The Accuracy Predicting The Test Set is: {accuracy_score(test_labels2, y_test) * 100} %\\n')\n",
    "conf_matrix2 = confusion_matrix(test_labels2, y_test)\n",
    "\n",
    "colLabels2 = ['Pred Plane', 'Pred Car', 'Pred Bird', 'Pred Cat', 'Pred Deer',\n",
    "             'Pred Dog', 'Pred Frog', 'Pred Horse', 'Pred Ship', 'Pred Truck']\n",
    "rowLabels2 = ['Actual Plane', 'Actual Car', 'Actual Bird', 'Actual Cat', 'Actual Deer',\n",
    "             'Actual Dog', 'Actual Frog', 'Actual Horse', 'Actual Ship', 'Actual Truck']\n",
    "\n",
    "fancyMatrix2 = pd.DataFrame(conf_matrix2, columns=colLabels2, index=rowLabels2,)\n",
    "display(fancyMatrix2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Method: Batch Normalization With Data Augmentation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define The Model With Normalization Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=x_train.shape[1:], use_bias=False))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Conv2D(32, (3, 3), padding='same', use_bias=False))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2))) # MaxPooling2D Layer or AveragePooling2D Layer\n",
    "\n",
    "model6.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model6.add(Flatten()) # Optional:Flattens 2D images to 1D Array\n",
    "model6.add(Dense(units=512))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Dropout(0.5)) # Helps to reduce overfitting\n",
    "model6.add(Dense(units=len(classes)))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Activation('softmax'))\n",
    "model6.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile The Model:\n",
    "\n",
    " - Still Using Same Parameters As Model 2\n",
    " - Increase Learning Rate 10x: 0.001 to 0.01\n",
    " - Increase Batch Size 4x: 16 to 64\n",
    " - Create A Data Generator For Real-Time Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path6 = '../assets/Lecture7/model3_ckpt.h5'\n",
    "callbacks_save6 = ModelCheckpoint(save_path6, monitor='val_loss', verbose=0, save_best_only=True, save_freq='epoch')\n",
    "\n",
    "opt6 = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model6.compile(loss='categorical_crossentropy', optimizer=opt6, metrics=['accuracy'])\n",
    "\n",
    "datagen6 = ImageDataGenerator(\n",
    "    #shear_range = -0.1,\n",
    "    #zoom_range = -0.1,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Quantities Required For Featurewise Normalization:\n",
    "\n",
    " - std deviation, mean & principal components if ZCA Whitening is Applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen6.fit(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit The Model Using Data Generator & Batch Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history6 = model6.fit(x_train, y_train_c, batch_size=64, \n",
    "                      epochs=50, verbose=1, validation_data=(x_val, y_val_c), \n",
    "                      callbacks=[callbacks_save6])\n",
    "\n",
    "plotEverything(history6)                      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Best Model & Evaluate On Held-Out Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6a = load_model(save_path6)\n",
    "\n",
    "score6a = model6a.evaluate(x_train, y_train_c)\n",
    "print(f'Total Loss From Training Set: {score6a[0]}')\n",
    "print(f'Accuracy From Training Set: {score6a[1]}\\n')\n",
    "\n",
    "score6b = model4a.evaluate(x_val, y_val_c)\n",
    "print(f'Total Loss From Validation Set: {score6b[0]}')\n",
    "print(f'Accuracy From Validation Set: {score6b[1]}\\n')\n",
    "\n",
    "score6c = model4a.evaluate(x_test, y_test_c)\n",
    "print(f'Total Loss From Testing Set: {score6c[0]}')\n",
    "print(f'Accuracy From Testing Set: {score6c[1]}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Class Of Held Out Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class3_prob = model6.predict(x_test)\n",
    "test_labels3 = np.argmax(test_class3_prob, axis=1)\n",
    "print(f'The Accuracy Predicting The Test Set is: {accuracy_score(test_labels3, y_test) * 100} %\\n')\n",
    "conf_matrix3 = confusion_matrix(test_labels2, y_test)\n",
    "\n",
    "colLabels3 = ['Pred Plane', 'Pred Car', 'Pred Bird', 'Pred Cat', 'Pred Deer',\n",
    "             'Pred Dog', 'Pred Frog', 'Pred Horse', 'Pred Ship', 'Pred Truck']\n",
    "rowLabels3 = ['Actual Plane', 'Actual Car', 'Actual Bird', 'Actual Cat', 'Actual Deer',\n",
    "             'Actual Dog', 'Actual Frog', 'Actual Horse', 'Actual Ship', 'Actual Truck']\n",
    "\n",
    "fancyMatrix3 = pd.DataFrame(conf_matrix3, columns=colLabels3, index=rowLabels3,)\n",
    "display(fancyMatrix3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 L2 Regularization:\n",
    "\n",
    " - https://keras.io/api/layers/regularizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = layers.Dense(\n",
    "    units=64, kernel_regularizer=regularizers.l1_l2(l1 = 1e-5, l2=1e-4),\n",
    "    bias_regularizer=regularizers.l2(1e-4),\n",
    "    activity_regularizer=regularizers.l2(1e-5)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator In Keras:\n",
    " \n",
    " - For small datasets loaded into memory\n",
    " - https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False, samplewise_center=False,\n",
    "    featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
    "    zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n",
    "    height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
    "    channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False,\n",
    "    vertical_flip=False, rescale=None, preprocessing_function=None,\n",
    "    data_format=None, validation_split=0.0, dtype=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
